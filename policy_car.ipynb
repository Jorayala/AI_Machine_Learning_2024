{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorayala/AI_Machine_Learning_2024/blob/main/policy_car.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "85dcd102a3fdb3e5988c564bda8846c2",
          "grade": false,
          "grade_id": "cell-95d1a4259c54b654",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1GzGPbW2AV-1"
      },
      "source": [
        "# Solución de MDPs por medio de iteración de políticas\n",
        "\n",
        "Para este laboratorio utilizaremos el método de iteración de políticas para solucionar el problema del motor del vehículo definido en el laboratorio de MDPs.\n",
        "\n",
        "Para ello utilizaremos el ambiente definido de los motores y construiremos el método de iteración de políticas para resolverlo.\n",
        "\n",
        "## Ambiente\n",
        "\n",
        "Para recordar la definición del ambiente se realiza en la clase `MDP`. Esta clase recibe como parámetro una codificación de los estados, la dimensión del MDP dada por la cantidad de estados en el ambiente (dados como una tupla) y el estado inicial.\n",
        "\n",
        "La codificación del ambiente se define como una tabla de tamaño `estados x acciones` que para cada uno de los estados (filas) contiene una lista de tuplas con la información de la probabilidad asociada a la acción de la columna, el estado de llegada de ejecutar la acción y la recompensa asociada a la ejecución de la acción. La codificación de la tabla se muestra a continuación, donde los estados son `cold=0`, `warm=1` y `overheated=2`.\n",
        "\n",
        "![Encoding_table](table.png)\n",
        "\n",
        "Las dimensiones del tablero se almacenan en los astributos `nrows` y `ncols` de la clase `MDP`.\n",
        "Los valores codificados en el tablero se almacenarán en dos atributos de la clase, un atributo `transitions` que mantiene la probabilidad de transición de un estado a otro dada una acción como una lista de tuplas `(probability, state)` y un atributo `rewards` que mantiene la información las recompensas obtenida en el paso entre estados definido como una lista `(reward, state)`.\n",
        "\n",
        "Adicionalmente la clase tiene los atributos `initial_state` y `state` que corresponden al estado inicial, dado por parámetro, y el estado actual del agente en el ambiente, inicializado en el estado inicial.\n",
        "\n",
        "Note también que el estado `overheated` no tienen ninguna acción asociada a él y por lo tanto lo consideramos como un estado final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "041b7bf3ad01da858d4f409ac0914a24",
          "grade": false,
          "grade_id": "cell-92af32cc5544794c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "E2-xXHUpAV-4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, table, dimensions, initial_state):\n",
        "        self.states = ['cold', 'warm', 'overheated']\n",
        "        #rows = estados, cols = acciones\n",
        "        self.nrows, self.ncols = dimensions\n",
        "        self.grid = table.copy()\n",
        "        self.initial_state = initial_state\n",
        "        self.state = initial_state\n",
        "        self.actions = ['slow', 'fast']\n",
        "        self.rewards = []\n",
        "        self.transitions = []\n",
        "\n",
        "        for i in range(self.nrows - 1):\n",
        "            row_rewards = []\n",
        "            row_transitions = []\n",
        "            for j in range(self.ncols):\n",
        "                transitions = self.grid[i][j]\n",
        "                col_rewards = []\n",
        "                col_transitions = []\n",
        "\n",
        "                for k in range(len(transitions)):\n",
        "                    prob, state, reward = transitions[k]\n",
        "                    col_rewards.append((reward, state))\n",
        "                    col_transitions.append((prob, state))\n",
        "\n",
        "                row_rewards.append(col_rewards)\n",
        "                row_transitions.append(col_transitions)\n",
        "\n",
        "            self.rewards.append(row_rewards)\n",
        "            self.transitions.append(row_transitions)\n",
        "\n",
        "\n",
        "    def get_current_state(self):\n",
        "        return self.state\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "\n",
        "        if state == 2:\n",
        "            return []\n",
        "\n",
        "        return self.actions\n",
        "\n",
        "    def get_possible_states(self, state, action):\n",
        "\n",
        "\n",
        "        probabilities_list = []\n",
        "        rewards_list = []\n",
        "        states_list = []\n",
        "\n",
        "        if state == 2:\n",
        "            return probabilities_list, rewards_list, states_list\n",
        "\n",
        "        action_i = self.actions.index(action)\n",
        "\n",
        "        transitions_for_action  = self.transitions[state][action_i]\n",
        "        rewards_for_action = self.rewards[state][action_i]\n",
        "\n",
        "        for i in range(len(transitions_for_action)):\n",
        "\n",
        "            prob, state = transitions_for_action[i]\n",
        "            reward, _ = rewards_for_action[i]\n",
        "            probabilities_list.append(prob)\n",
        "            states_list.append(state)\n",
        "            rewards_list.append(reward)\n",
        "\n",
        "        return probabilities_list, rewards_list, states_list\n",
        "\n",
        "    def do_action(self, action):\n",
        "        if self.is_terminal():\n",
        "            raise SyntaxError('Cannot reach actions from state overheated')\n",
        "\n",
        "        state_i = self.state\n",
        "        action_i = self.actions.index(action)\n",
        "        tr_st = self.transitions[state_i][action_i]\n",
        "        rw_st = self.rewards[state_i][action_i]\n",
        "        sel_event = -1\n",
        "\n",
        "        if len(tr_st) > 1:\n",
        "            rn = random.uniform(0, 1)\n",
        "            acc_prob = 0\n",
        "            for i in range(len(tr_st)):\n",
        "                p, s = tr_st[i]\n",
        "                acc_prob += p\n",
        "\n",
        "                if rn <= p:\n",
        "                    sel_event = i\n",
        "\n",
        "        r, s = rw_st[sel_event]\n",
        "\n",
        "        self.state = s\n",
        "\n",
        "        return r, self.state\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.initial_state\n",
        "\n",
        "    def is_terminal(self):\n",
        "\n",
        "        if self.state == 2:\n",
        "            return True\n",
        "\n",
        "        if len(self.get_possible_actions(self.state)) == 0:\n",
        "            return True\n",
        "\n",
        "        else:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "648de7caacfb9416731f41cf69117642",
          "grade": false,
          "grade_id": "cell-7631be86d36835a7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZbrvPyGbAV-6"
      },
      "source": [
        "## Creación del agente\n",
        "\n",
        "#### 1. definición de la clase de iteración de políticas\n",
        "\n",
        "Implemente la clase `PolicyIteration` que define cinco atributos:\n",
        "\n",
        "- `mdp` que corresponde al MDP a resolver\n",
        "- `discount` que corresponde al factor de decuento a utilizar.\n",
        "- `iterations` que corresponde a el número de iteraciones a realizar\n",
        "- `values` que corresponde a un mapa con los valores calculados para los estados del MDP. Los estados se definen como una tupla de los valores posibles del estado. En el caso de Gridworld, la tupla es la posición de cada casilla. Inicialmente definiremos el mapa como un mapa vacío, el cual poblaremos con los estados descubiertos durante cada iteración.\n",
        "- `policy` que corresponde a un mapa con los nombres de las acciones a ejecutar para cada uno de los estados del ambiente. La política se debe inicializar con una acción aleatoria para cada estado. Si el estado no tiene acciones posibles, su política se debe inicializar en `None`.\n",
        "\n",
        "Al momento de crear la clase `PolicyIteration`, esta debe recibir por parámetro, la definición del MDP `MDP`, el valor de descuento `discount` (inicializado por defecto en 0.9 si no se pasa ningún valor) y la cantidad de iteraciones a ejecutar `iterations` (con un valor de 30 por defecto, si no se pasa ningún valor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e5681eaa0e9743896208db5f6feacb05",
          "grade": false,
          "grade_id": "cell-61243e8a1a36a056",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "A4mFrurnAV-7"
      },
      "outputs": [],
      "source": [
        "# Definición de las librerías a utilizar\n",
        "\n",
        "class PolicyIteration():\n",
        "    def __init__(self, mdp: MDP, discount: float = 0.9, iterations: int = 64):\n",
        "        self.mdp = mdp\n",
        "        self.discount = discount\n",
        "        self.iterations = iterations\n",
        "        self.last_values = {}\n",
        "        self.values = {}  # {(i,j): 0 for i in range(self.mdp.nrows) for j in range(self.mdp.ncols)}\n",
        "        self.policy = {}\n",
        "        self.last_policy = {}\n",
        "        for i in range(mdp.nrows):\n",
        "\n",
        "            state = i\n",
        "            actions = self.mdp.get_possible_actions(state)\n",
        "            if not actions:\n",
        "                self.policy[state] = None\n",
        "            else:\n",
        "                self.policy[state] = random.choice(actions)\n",
        "\n",
        "    def set_policy(self, policy: list[str]):\n",
        "        # Función de actualización de la política usada para las pruebas\n",
        "        self.policy = policy\n",
        "\n",
        "    def get_policy(self, state: int) -> str:\n",
        "        # your code here\n",
        "        return self.policy[state]\n",
        "\n",
        "    def get_value(self, state: int) -> float:\n",
        "        # your code here\n",
        "        if self.values.get(state) == None:\n",
        "            self.values[state] = 0\n",
        "        return self.values[state]\n",
        "\n",
        "    def get_action(self, state: int) -> str:\n",
        "        # your code here\n",
        "        return self.policy[state]\n",
        "\n",
        "    def compute_new_action(self, state: int) -> str:\n",
        "        # your code here\n",
        "        best_action = self.policy[state]\n",
        "        best_value = self.get_value(state)\n",
        "        max_value, max_action = self.compute_new_value(state)\n",
        "        if max_value > best_value:\n",
        "            best_value = max_value\n",
        "            best_action = max_action\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def value_evaluation(self, state: int, action: str) -> float:\n",
        "        # your code here\n",
        "        if state != 2:\n",
        "            probs, rewards, states = self.mdp.get_possible_states(state, action)\n",
        "            val = 0\n",
        "            for i in range(len(probs)):\n",
        "                val += probs[i] * (rewards[i] + self.discount * self.get_value(states[i]))\n",
        "            return val\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def compute_new_value(self, state: int) -> tuple[float, str]:\n",
        "        # your code here\n",
        "        if state != 2:\n",
        "            max_val = float('-inf')\n",
        "            action = \"\"\n",
        "            for a in self.mdp.get_possible_actions(state):\n",
        "                probs, rewards, states = self.mdp.get_possible_states(state, a)\n",
        "                val = 0\n",
        "                for i in range(len(probs)):\n",
        "                    val += probs[i] * (rewards[i] + self.discount * self.get_value(states[i]))\n",
        "                if val >= max_val:\n",
        "                    max_val = val\n",
        "                    action = a\n",
        "            return max_val, action\n",
        "        else:\n",
        "            return 0, self.policy[state]\n",
        "\n",
        "    def policy_evaluation(self, new_policy: dict[int, str]) -> bool:\n",
        "        # your code here\n",
        "        print(new_policy)\n",
        "\n",
        "        improvement = False\n",
        "        for state in range(len(self.mdp.states)):\n",
        "            print(self.policy[state])\n",
        "            if self.policy[state] != new_policy[state]:\n",
        "                improvement = True\n",
        "                return improvement\n",
        "        return improvement\n",
        "\n",
        "    def policy_iteration(self) -> tuple[bool, dict[int, str]]:\n",
        "        # your code here\n",
        "        new_values = {}\n",
        "        for state in range(len(self.mdp.states)):\n",
        "            new_values[state] = self.get_value(state)\n",
        "            new_values[state] = self.value_evaluation(state, self.policy[state])\n",
        "        self.values = new_values\n",
        "        # Mejora\n",
        "        new_policy = {}\n",
        "        for state in range(len(self.mdp.states)):\n",
        "            new_policy[state] = self.policy[state]\n",
        "            new_policy[state] = self.compute_new_action(state)\n",
        "        if self.policy_evaluation(new_policy):\n",
        "            # print(\"Convergencia en la iteracion:\",it)\n",
        "            self.policy = new_policy\n",
        "            return False, self.policy\n",
        "        return True, self.policy\n",
        "\n",
        "    def run_policy_iteration(self) -> tuple[dict[float], dict[str]]:\n",
        "        done = False\n",
        "        i = 1\n",
        "        policy = {}\n",
        "        while not done and i <= self.iterations:\n",
        "            done, policy = self.policy_iteration()\n",
        "            i += 1\n",
        "        if done:\n",
        "            print(f\"La política converge en {i} iteraciones\")\n",
        "            print(f\"Política : {policy}\")\n",
        "        return self.values, self.policy\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "100418b154bbb8a058bafd20c2b8865e",
          "grade": false,
          "grade_id": "cell-174f2156168ff74b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-i3gaTnaAV-7"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "table = [[[(1, 0, 1)], [(0.5, 0, 2), (0.5, 1, 2)]],\n",
        "         [[(0.5, 0, 1), (0.5, 1, 1)], [(1, 2, -10)]],\n",
        "         [None, None]]\n",
        "\n",
        "env = MDP(table, (3, 2), 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f43ebc7d6f8a6456a4dac833d779d658",
          "grade": true,
          "grade_id": "cell-076d44492595eeee",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "QW5MvD27AV-7"
      },
      "outputs": [],
      "source": [
        "#Pruebas estructura del agente\n",
        "\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "try:\n",
        "    a.mdp\n",
        "    assert type(a.mdp) is MDP, \"El tipo del mdp debe ser MDP (el tipo de la clase)\"\n",
        "except:\n",
        "    print(\"El atributo mdp no está definido\")\n",
        "try:\n",
        "    a.discount\n",
        "    assert type(a.discount) is float, \"El tipo del factor de descuento debe ser float\"\n",
        "    assert 0 < a.discount and a.discount <=1, \"El factor de descuento debe ser un valor entre 0 (excluido) y 1 (incluido)\"\n",
        "except:\n",
        "    print(\"El atributo discount no está definido\")\n",
        "try:\n",
        "    a.iterations\n",
        "    assert type(a.iterations) is int, \"El tipo de la cantidad de iteraciones debe ser entero\"\n",
        "except:\n",
        "    print(\"El atributo iterations no está definido\")\n",
        "\n",
        "try:\n",
        "    a.values\n",
        "    assert type(a.values) is dict or type(a.values) is dict[int,float], \"El tipo del mapa de valeres debe ser dict (el tipo de los mapas en python). El mapa puede estar isntanciado en su llave y valor si fue inicializado previamente\"\n",
        "except:\n",
        "    print(\"El atributo values no está definido\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e64d5ab6ecfed2189d68e27b4869b0ec",
          "grade": false,
          "grade_id": "cell-7e9c3c0af0b464ec",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PGV0RLb9AV-8"
      },
      "source": [
        "#### 2. Política del agente\n",
        "\n",
        "Implemente la función `get_policy` que recibe un estado por parámetro y retorna el nombre de la acción (política) actual para dicho estado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c046546d806fe5cbca05da9b7c9cbd54",
          "grade": true,
          "grade_id": "cell-a3f0c350a72c3b41",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yAjMqVggAV-8"
      },
      "outputs": [],
      "source": [
        "#Pruebas política\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "try:\n",
        "    a.get_policy\n",
        "except:\n",
        "    print(\"La función get_policy no está definida\")\n",
        "\n",
        "assert type(a.get_policy(1)) is str, \"La función get_policy debe retornan un string con el nombre de la acción a ejecutar en el estado dado, o None si del estado no se puede ejecutar ninguna acción\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "34da1b93243e3a40bd1100be9cc22956",
          "grade": false,
          "grade_id": "cell-0d1dfd8349ffd766",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IRaWEZ18AV-8"
      },
      "source": [
        "#### 3. Valores de estados\n",
        "\n",
        "Defina la función `get_value` que recibe un estado (el entero representando el estado) como parámetro y retorna el valor correspondiente para dicho estado. Si el estado no ha sido visitado, la función debe retornar 0, el valor inicial para el estado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "681dac012ad2154000aeeaa7c6f3c17a",
          "grade": true,
          "grade_id": "cell-070665537f19fe58",
          "locked": true,
          "points": 6,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yTl-it1pAV-8"
      },
      "outputs": [],
      "source": [
        "#Pruebas de get_value\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "\n",
        "try:\n",
        "    a.get_value\n",
        "except:\n",
        "    print(\"La función get_value no está definida\")\n",
        "\n",
        "assert type(a.get_value(0)) is int or type(a.get_value(0)) is float, f\"La función debe retornar un valor entero o un flotante\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3c644db2454e4b37daa06967fecfc9e5",
          "grade": false,
          "grade_id": "cell-85b0a5ce579ed268",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "P9pWVSTAAV-9"
      },
      "source": [
        "#### 3. Evaluación de valores\n",
        "\n",
        "Defina la función `value_evaluation` que tiene como proposito calcular el valor de un estado (sin modificar el atributo de valores), dada una acción.\n",
        "Esta función recibe por parámetro un estado y una acción (el string del nombre) a ejecutar en el estado (entero en este caso) y retorna el valor calculado para dicho estado.\n",
        "\n",
        "Notas:\n",
        "- El valor por defecto de todos los estados debe ser 0.\n",
        "- Para poder realizar este cálculo requerimos la nueva función del ambiente `get_possible_states`. La nueva función recibe el nombre de la acción a ejecutar para el estado actual del ambiente y retorna una lista con las probabilidades de transiciones entre los estados, una lista de recompensas y una lista de los estados de llegada del estado actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cc8a4538da0bcdf2e15d831e08efa41f",
          "grade": true,
          "grade_id": "cell-33f712394666172b",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "QOQUZ1pdAV-9"
      },
      "outputs": [],
      "source": [
        "#Pruebas de value_evaluation\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "try:\n",
        "    a.value_evaluation\n",
        "except:\n",
        "    print(\"La función value_evaluation no está definida\")\n",
        "\n",
        "assert type(a.value_evaluation(0, 'fast')) is int or type(a.value_evaluation(0,'fast')) is float, f\"La función value_evaluation debe retornar un valor de tipo entero o flotante, no {type(a.value_evaluation(0, 'fast'))}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "65ac2a839a68107942f8b1325a4a2faa",
          "grade": false,
          "grade_id": "cell-d2a6be582243b83a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4yPz59dLAV--"
      },
      "source": [
        "#### 4. Cálculo de valor\n",
        "\n",
        "Defina la función `compute_new_value` que de forma similar a `value_evaluation` calcula el valor para el estado. La diferencia con la función anterior, es que `compute_new_value` calcula el valor máximo de todas las acciones posibles, dado un estado y no solo el valor de una única acción.\n",
        "Esta función recibe un estado como parámetro y calcula el nuevo valor para el estado siguiendo la fórmula de iteración de políticas. Esta función calcula el nuevo valor para el estado actual del agente (`s`) a partir de las acciones, las recompensas y los valores de los posibles estados de llegada (`s'`) de acuerdo a la función de transición (o ruido), obteniendo el valor máximo de todas las posibles acciones en el estado de entrada.\n",
        "\n",
        "Adicional al valor máximo, esta función debe retornar el nombre de la acción que nos lleva a ese máximo valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "847ed028c39c5a4f0e60f14d9ab5af5f",
          "grade": true,
          "grade_id": "cell-e3c9ea9ec965a4a8",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eEl1_DKGAV--"
      },
      "outputs": [],
      "source": [
        "#Pruebas de compute_new_value\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 50)\n",
        "\n",
        "try:\n",
        "    a.compute_new_value\n",
        "except:\n",
        "    print(\"La función compute_new_value no está definida\")\n",
        "\n",
        "assert type(a.compute_new_value(0)) is tuple, \"La función compute_new_value debe retornar una tupla (de tipo entero o flotante), no {type(a.compute_new_value(0))}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5b0728368f4ddf62d9ed876a525bcd3c",
          "grade": false,
          "grade_id": "cell-74cbf85b9d7d1301",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Avp5lsfkAV-_"
      },
      "source": [
        "#### 5. Acción a ejecutar\n",
        "\n",
        "Defina la función `get_action` que retorna la acción a tomar para un estado dado por parámetro, de acuerdo a los valores de los estados aledaños. Esta función debe observar el posible valor a obtener para cada una de sus acciones posibles y retornar aquella acción (su nombre) que lleva al mejor valor.\n",
        "Tenga en cuenta que la evaluación de las acciones no debe modificar el valor actual del estado, o tener un efecto visible sobre el ambiente o el agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "79de0aa1248b7abf89e1800af59f162c",
          "grade": true,
          "grade_id": "cell-289a68d5da2b8fa5",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "wiWZubl9AV-_"
      },
      "outputs": [],
      "source": [
        "#Pruebas de get_action\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "\n",
        "try:\n",
        "    a.get_action\n",
        "except:\n",
        "    print(\"La función get_action no está definida\")\n",
        "\n",
        "assert type(a.get_action(0)) is str, \"La función get_action debe retornar el nombre de una acción tipo string\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c7ecc051a10e0a70eeaef7aab19a9b43",
          "grade": false,
          "grade_id": "cell-21ce944b14e2b667",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LuZO4FmAAV-_"
      },
      "source": [
        "#### 6. Calculo de acciones\n",
        "\n",
        "Implemente la función `compute_new_action` que retorna la nueva mejor acción a realizar luego de ejecutar el cálculo del nuevo valor para un estado. La función recibe un estado como parámetro y retorna la mejor acción a ejecutar para ese estado (actualizando la política).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "03c3bba02dc9e93a2c4fd3872a5c46a2",
          "grade": true,
          "grade_id": "cell-3109672f07675f22",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2Znk-XCXAV-_"
      },
      "outputs": [],
      "source": [
        "#Pruebas de compute_new_action\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "try:\n",
        "    a.compute_new_action\n",
        "except:\n",
        "    print(\"La función compute_new_action no está definida\")\n",
        "\n",
        "assert type(a.compute_new_action(0)) is str, \"La función compute_new_action debe retornar un str (o None si no hay acción posible)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ebf983732c5c39076e03e494d0b8ac63",
          "grade": false,
          "grade_id": "cell-f4775c8463d8af59",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "QPWlSSHFAV-_"
      },
      "source": [
        "#### 7. Evaluación de políticas\n",
        "\n",
        "Implemente la función `policy_evaluation` recibe la nueva política calculada como parámetro y evalua si existió algún cambio. En caso de tener un cambio con la política actual, la función retorna `True`, de lo contrario retorna `False`.\n",
        "\n",
        "Tenga en cuenta que la evaluación de la política se debe hacer sobre los estados del ambiente, en este caso `cold`, `hot`, `overheated`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f4d5e8f43a3c8a84410cc82a2d319f85",
          "grade": true,
          "grade_id": "cell-72a4b3fe96abc7a8",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB0o8BthAV-_",
        "outputId": "ff52036b-3234-44d6-9737-175071cf840a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'slow', 1: 'slow', 2: None}\n",
            "slow\n",
            "slow\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#Pruebas de policy_evaluation\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 50)\n",
        "\n",
        "try:\n",
        "    a.policy_evaluation\n",
        "except:\n",
        "    print(\"La función policy_evaluation no está definida\")\n",
        "\n",
        "assert type(a.policy_evaluation(a.policy)) is bool, \"La función policy_evaluation debe retornar un booleano\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "46562ff9390742edaeb9aee47199f699",
          "grade": false,
          "grade_id": "cell-10dfe2cfa1f845b1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PJ0tLqwWAV_A"
      },
      "source": [
        "#### 8. Iteración de políticas\n",
        "\n",
        "Implemente la función `policy_iteration` encargada de ejecutar la iteración de políticas en tres pasos.\n",
        "1. Primero se realiza el cálculo de los nuevos valores. Esto quiere decir que para cada uno de los estados del ambiente se calcula su nuevo valor utilizando la política actual (con la función `value_evaluation`).\n",
        "2. Segundo, para los nuevos valores se revisa la mejora política posible, calculando la mejor acción posible para cada uno de los estados (usando función `compute_new_action`).\n",
        "3. Tercero, se evalua la política calculada de las nuevas acciones en el paso anterior, actualizando la políticia si se encontraron mejores acciones. Si no hay una mejor acción, la función debe retornar la política calculada y un indicador (booleano) que hay convergencia para detener la iteración.\n",
        "\n",
        "La función debe retornar `True` si la política converge y `False` si no lo hace, junto con la política calculada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8a1ef8bd85920fc7d9cb8be471410e5",
          "grade": true,
          "grade_id": "cell-b38c43e9564d2f49",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IbicNIJAV_A",
        "outputId": "40fe7013-5af9-4ad8-888e-ad7750675c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'fast', 1: 'slow', 2: None}\n",
            "slow\n"
          ]
        }
      ],
      "source": [
        "#Pruebas de policy_iteration\n",
        "\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "### BEGIN TESTS\n",
        "try:\n",
        "    a.policy_iteration\n",
        "except:\n",
        "    print(\"La función policy_iteration no está definida\")\n",
        "\n",
        "assert type(a.policy_iteration()) is tuple, \"La iteración de política debe retornar una tupla con un booleano y un mapa con la política\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9bf09b8fab92ee9a8c7dca90483dd9f6",
          "grade": false,
          "grade_id": "cell-3cbf1a0acaf17282",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "GGiej8UrAV_A"
      },
      "source": [
        "\n",
        "#### Ejecución del agente\n",
        "\n",
        "Finalmente definimos la función `run_policy_iteration` que no recibe ningún parámatetro y ejecuta el algoritmo de solución del MDP.\n",
        "\n",
        "Esta función ejecuta el número de iteraciones dadas a la clase. Para cada una de las iteraciones se debe calcular el nuevo valor para cada uno de los estados del ambiente y la nueva política.\n",
        "\n",
        "Tenga en cuenta que el cálculo de los nuevos valores se debe realizar con los valores anteriores y únicamente se deben actualizar los valores cuando se actualicen los valores para todos los estados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f87fe7b2c756d10df8c6c9a7e3266886",
          "grade": false,
          "grade_id": "cell-4d97387d04b7c3d2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0-CeznPAV_A",
        "outputId": "07622cd7-fd4e-4e5a-9048-3f4ff89084db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'fast', 1: 'slow', 2: None}\n",
            "slow\n",
            "{0: 'fast', 1: 'slow', 2: None}\n",
            "fast\n",
            "slow\n",
            "None\n",
            "La política converge en 3 iteraciones\n",
            "Política : {0: 'fast', 1: 'slow', 2: None}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({0: 2.8, 1: 1.8, 2: 0}, {0: 'fast', 1: 'slow', 2: None})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# your code here\n",
        "a = PolicyIteration(env, 0.8, 10)\n",
        "\n",
        "a.run_policy_iteration()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}