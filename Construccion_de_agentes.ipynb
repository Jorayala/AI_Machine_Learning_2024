{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaUi6/Z59tg/+vMEOtj1sG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorayala/AI_Machine_Learning_2024/blob/main/Construccion_de_agentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pnceYQdRo33",
        "outputId": "5e6cc984-a8cb-4beb-9bf5-3b966340aa17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1:\n",
            "Q-Table after episode 1:\n",
            "[[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [1. 0.]\n",
            " [0. 0.]]\n",
            "\n",
            "Episode 2:\n",
            "Q-Table after episode 2:\n",
            "[[0.   0.  ]\n",
            " [0.   0.  ]\n",
            " [0.06 0.  ]\n",
            " [1.9  0.  ]\n",
            " [0.   0.  ]]\n",
            "\n",
            "Episode 3:\n",
            "Q-Table after episode 3:\n",
            "[[0.     0.    ]\n",
            " [0.0036 0.    ]\n",
            " [0.168  0.    ]\n",
            " [2.71   0.    ]\n",
            " [0.     0.    ]]\n",
            "\n",
            "Episode 4:\n",
            "Q-Table after episode 4:\n",
            "[[4.104e-04 0.000e+00]\n",
            " [1.332e-02 1.296e-05]\n",
            " [3.138e-01 0.000e+00]\n",
            " [3.439e+00 0.000e+00]\n",
            " [0.000e+00 0.000e+00]]\n",
            "\n",
            "Episode 5:\n",
            "Q-Table after episode 5:\n",
            "[[1.16856e-03 0.00000e+00]\n",
            " [3.08160e-02 1.29600e-05]\n",
            " [4.88760e-01 0.00000e+00]\n",
            " [4.09510e+00 0.00000e+00]\n",
            " [0.00000e+00 0.00000e+00]]\n",
            "\n",
            "Episode 6:\n",
            "Q-Table after episode 6:\n",
            "[[2.900664e-03 0.000000e+00]\n",
            " [5.706000e-02 1.296000e-05]\n",
            " [6.855900e-01 0.000000e+00]\n",
            " [4.685590e+00 0.000000e+00]\n",
            " [0.000000e+00 0.000000e+00]]\n",
            "\n",
            "Episode 7:\n",
            "Q-Table after episode 7:\n",
            "[[6.0341976e-03 0.0000000e+00]\n",
            " [9.2489400e-02 1.2960000e-05]\n",
            " [8.9816640e-01 0.0000000e+00]\n",
            " [5.2170310e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00]]\n",
            "\n",
            "Episode 8:\n",
            "Q-Table after episode 8:\n",
            "[[1.09801418e-02 3.62051856e-04]\n",
            " [1.37130444e-01 1.29600000e-05]\n",
            " [1.12137162e+00 0.00000000e+00]\n",
            " [5.69532790e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Episode 9:\n",
            "Q-Table after episode 9:\n",
            "[[1.81099543e-02 3.62051856e-04]\n",
            " [1.90699697e-01 1.29600000e-05]\n",
            " [1.35095413e+00 0.00000000e+00]\n",
            " [6.12579511e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Episode 10:\n",
            "Q-Table after episode 10:\n",
            "[[2.77409407e-02 3.62051856e-04]\n",
            " [2.52686975e-01 1.29600000e-05]\n",
            " [1.58340643e+00 0.00000000e+00]\n",
            " [6.51321560e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class Environment:\n",
        "    # Definición del ambiente sin los métodos que serán manejados por Learner.\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.goal_state = size - 1\n",
        "\n",
        "    def start(self):\n",
        "        # Retorna el estado inicial.\n",
        "        return 0\n",
        "\n",
        "    def end(self):\n",
        "        # Retorna el estado objetivo.\n",
        "        return self.goal_state\n",
        "\n",
        "class Agent:\n",
        "    # Agente que puede moverse hacia adelante o hacia atrás.\n",
        "    def __init__(self, right_bound):\n",
        "        self.state = 0\n",
        "        self.right_bound = right_bound\n",
        "        self.actions = ['forward', 'back']\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def move(self, action):\n",
        "        # Acciones para mover al agente.\n",
        "        if action == 'forward' and self.state < self.right_bound:\n",
        "            self.state += 1\n",
        "        elif action == 'back' and self.state > 0:\n",
        "            self.state -= 1\n",
        "        return self.state\n",
        "\n",
        "    def get_action_name(self, action_number):\n",
        "        # Retorna el nombre de la acción basada en el número de acción.\n",
        "        return self.actions[action_number]\n",
        "\n",
        "class Learner:\n",
        "    # Clase Learner que manejará el aprendizaje del agente.\n",
        "    def __init__(self, agent, env, alpha=0.1, gamma=0.6, epsilon=0.1):\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros((env.size, len(agent.actions)))\n",
        "\n",
        "    def _init_table(self):\n",
        "        # Inicializa la tabla Q con ceros.\n",
        "        self.q_table = np.zeros((self.env.size, len(self.agent.actions)))\n",
        "\n",
        "    def run(self):\n",
        "        done = False\n",
        "        while not done:\n",
        "            current_state = self.agent.state\n",
        "            if random.uniform(0, 1) < self.epsilon:\n",
        "                # Exploración: elige una acción aleatoria.\n",
        "                action = random.choice(self.agent.actions)\n",
        "            else:\n",
        "                # Explotación: elige la mejor acción basada en q_table.\n",
        "                action = self.agent.get_action_name(np.argmax(self.q_table[current_state]))\n",
        "            next_state = self.agent.move(action)\n",
        "            reward, done = self.get_reward(next_state)\n",
        "            old_value = self.q_table[current_state, self.agent.actions.index(action)]\n",
        "            next_max = np.max(self.q_table[next_state])\n",
        "            new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
        "            self.q_table[current_state, self.agent.actions.index(action)] = new_value\n",
        "\n",
        "    def random_action(self):\n",
        "        return random.choice(range(len(self.agent.actions)))\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        if state == self.env.end():\n",
        "            return 10, True\n",
        "        else:\n",
        "            return 0, False\n",
        "\n",
        "def main():\n",
        "    # Configuración y ejecución del entorno de aprendizaje.\n",
        "    env = Environment(size=5)\n",
        "    agent = Agent(right_bound=env.end())\n",
        "    learner = Learner(agent, env)\n",
        "\n",
        "    episodes = 10\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode {episode + 1}:\")\n",
        "        agent.reset()\n",
        "        learner.run()\n",
        "        print(f\"Q-Table after episode {episode + 1}:\\n{learner.q_table}\\n\")\n",
        "\n",
        "main()\n"
      ]
    }
  ]
}