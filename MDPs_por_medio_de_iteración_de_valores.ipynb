{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KYVmNJgD0dZuKZTKz7w5hgHnBWW33Crj",
      "authorship_tag": "ABX9TyMOzjakoWbRziG/5tXyD7a6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorayala/AI_Machine_Learning_2024/blob/main/MDPs_por_medio_de_iteraci%C3%B3n_de_valores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Definicion de la clase principal\n",
        "class MDP:\n",
        "    def __init__(self, table, dimensions, initial_state):\n",
        "        # your code here\n",
        "        self.nrows, self.ncols = dimensions\n",
        "        self.table = table\n",
        "        self.initial_state = initial_state\n",
        "        self.state = initial_state\n",
        "        self.actions = ['slow', 'fast']\n",
        "        # Inicializar los atributos transitions y rewards como listas de listas\n",
        "        self.transitions = [[[] for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
        "        self.rewards = [[[] for _ in range(self.ncols)] for _ in range(self.nrows)]\n",
        "\n",
        "        # Procesar la tabla para separar transiciones y recompensas\n",
        "        for i in range(self.nrows):\n",
        "            for j in range(self.ncols):\n",
        "                if table[i][j] is not None:\n",
        "                    for prob, next_state, reward in table[i][j]:\n",
        "                        self.transitions[i][j].append((prob, next_state))\n",
        "                        self.rewards[i][j].append((reward, next_state))\n",
        "\n",
        "\n",
        "    def get_current_state(self):\n",
        "        # your code here\n",
        "        return self.state\n",
        "\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        # your code here\n",
        "        possible_actions = []\n",
        "        for action_index in range(self.ncols):\n",
        "            if self.table[state][action_index] is not None:\n",
        "                possible_actions.append(self.actions[action_index])\n",
        "        return possible_actions\n",
        "\n",
        "\n",
        "    def get_possible_states(self, action):\n",
        "        action_index = self.actions.index(action)\n",
        "        transitions = self.table[self.state][action_index]\n",
        "        if transitions is None:\n",
        "            return [], [], []\n",
        "        probabilities, rewards, next_states = zip(*transitions)\n",
        "        return probabilities, rewards, next_states\n",
        "\n",
        "    def do_action(self, action):\n",
        "        # your code here\n",
        "        if self.is_terminal():\n",
        "            print(\"Se daño el carro.\")\n",
        "            return None, self.state\n",
        "\n",
        "        if action not in self.actions:\n",
        "            print(\"Acción no correcta.\")\n",
        "            return None, self.state\n",
        "\n",
        "        action_index = self.actions.index(action)\n",
        "        transitions = self.table[self.state][action_index]\n",
        "\n",
        "        if transitions is None:\n",
        "            print(\"No hay transiciones para esta accion.\")\n",
        "            return None, self.state\n",
        "\n",
        "        # probabilidades acumuladas\n",
        "        cumulative_probabilities = []\n",
        "        cumulative = 0.0\n",
        "        for t in transitions:\n",
        "            cumulative += t[0]\n",
        "            cumulative_probabilities.append(cumulative)\n",
        "        # transición basada en probabilidades\n",
        "        rand = random.random()\n",
        "        for i, cumulative_prob in enumerate(cumulative_probabilities):\n",
        "            if rand <= cumulative_prob:\n",
        "                next_state = transitions[i][1]\n",
        "                reward = transitions[i][2]\n",
        "                break\n",
        "\n",
        "        self.state = next_state\n",
        "        return reward, next_state\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # your code here\n",
        "        self.state = self.initial_state\n",
        "\n",
        "\n",
        "    def is_terminal(self):\n",
        "        # your code here\n",
        "        return len(self.get_possible_actions(self.state)) == 0\n",
        "\n"
      ],
      "metadata": {
        "id": "Ss9ki21pzLIM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FYm-qgVmzCkf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "class ValueIteration:\n",
        "    def __init__(self, mdp, discount=0.9, iterations=30):\n",
        "\n",
        "        # Validaciones de tipos y valores\n",
        "        if not isinstance(mdp, MDP):\n",
        "            raise TypeError(\"mdp debe ser una instancia de la clase MDP\")\n",
        "        if not isinstance(discount, float) or discount <= 0 or discount > 1:\n",
        "            raise ValueError(\"discount debe ser un float entre 0 (exclusivo) y 1 (inclusivo)\")\n",
        "        if not isinstance(iterations, int) or iterations <= 0:\n",
        "            raise ValueError(\"iterations debe ser un entero positivo\")\n",
        "\n",
        "        self.mdp = mdp\n",
        "        self.discount = discount\n",
        "        self.iterations = iterations\n",
        "        self.values = {}  # Inicializar diccionario vacío\n",
        "\n",
        "    def get_value(self, state):\n",
        "\n",
        "        return self.values.get(state, 0)\n",
        "\n",
        "    def compute_new_value(self, state):\n",
        "\n",
        "        # Si es estado terminal, retornar 0\n",
        "        if not self.mdp.get_possible_actions(state):\n",
        "            return 0\n",
        "\n",
        "        max_value = float('-inf')\n",
        "\n",
        "        # Para cada acción posible en el estado\n",
        "        for action in self.mdp.get_possible_actions(state):\n",
        "            value = 0\n",
        "            action_index = self.mdp.actions.index(action)\n",
        "\n",
        "            # Obtener transiciones para esta acción\n",
        "            transitions = self.mdp.table[state][action_index]\n",
        "            if transitions:\n",
        "                # Calcular valor esperado según la ecuación de Bellman\n",
        "                for prob, next_state, reward in transitions:\n",
        "                    value += prob * (reward + self.discount * self.get_value(next_state))\n",
        "\n",
        "            max_value = max(max_value, value)\n",
        "\n",
        "        return max_value\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        best_value = float('-inf')\n",
        "        best_action = None\n",
        "        possible_actions = self.mdp.get_possible_actions(state)\n",
        "        for action in possible_actions:\n",
        "            probabilities, rewards, next_states = self.mdp.get_possible_states(action)\n",
        "            value = 0\n",
        "            for prob, reward, next_state in zip(probabilities, rewards, next_states):\n",
        "                value += prob * (reward + self.discount * self.get_value(next_state))\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def run_value_iteration(self):\n",
        "        \"\"\"\n",
        "        Ejecuta el algoritmo de iteración de valores\n",
        "        \"\"\"\n",
        "        for i in range(self.iterations):\n",
        "            new_values = {}\n",
        "\n",
        "            # Actualizar valores para todos los estados\n",
        "            for state in range(self.mdp.nrows):\n",
        "                new_values[state] = self.compute_new_value(state)\n",
        "\n",
        "            self.values = new_values\n",
        "            print(f\"Iteración {i+1}: {self.values}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos un ejemplo del ambiente para probar la clase ValueIteration\n",
        "table = [[[(1,0,1)], [(0.5, 0, 2), (0.5,1,2)]],\n",
        "         [[(0.5, 0, 1), (0.5,1,1)], [(1, 2, -10)]],\n",
        "         [None, None]]\n",
        "\n",
        "env = MDP(table, (3,2), 0)"
      ],
      "metadata": {
        "id": "82VT0Y39zRY3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    a.mdp\n",
        "    assert type(a.mdp) is MDP, \"El tipo del mdp debe ser MDP (el tipo de la clase)\"\n",
        "except:\n",
        "    print(\"El atributo mdp no está definido\")\n",
        "try:\n",
        "    a.discount\n",
        "    assert type(a.discount) is float, \"El tipo del factor de descuento debe ser float\"\n",
        "    assert 0 < a.discount and a.discount <=1, \"El factor de descuento debe ser un valor entre 0 (excluido) y 1 (incluido)\"\n",
        "except:\n",
        "    print(\"El atributo discount no está definido\")\n",
        "try:\n",
        "    a.iterations\n",
        "    assert type(a.iterations) is int, \"El tipo de la cantidad de iteraciones debe ser entero\"\n",
        "except:\n",
        "    print(\"El atributo iterations no está definido\")\n",
        "\n",
        "try:\n",
        "    a.values\n",
        "    assert type(a.values) is dict or type(a.values) is dict[int,float], \"El tipo del mapa de valores debe ser dict (el tipo de los mapas en python). El mapa puede estar instanciado en su llave y valor si fue inicializado previamente\"\n",
        "except:\n",
        "    print(\"El atributo values no está definido\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnlaNDExzWI-",
        "outputId": "d726e931-f2f2-4080-cfd2-a6ceb3863682"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El atributo mdp no está definido\n",
            "El atributo discount no está definido\n",
            "El atributo iterations no está definido\n",
            "El atributo values no está definido\n"
          ]
        }
      ]
    }
  ]
}